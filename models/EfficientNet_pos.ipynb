{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca07fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "\n",
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "\n",
    "# dataset and transformation\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# display images\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from torchmetrics import F1Score\n",
    "#from pytorchtools import EarlyStopping\n",
    "import time\n",
    "import copy\n",
    "import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c191beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc260e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--aff_path', type=str, default='datasets/AfectNet/', help='AfectNet dataset path.')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='Batch size.')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='Initial learning rate for adam.')\n",
    "parser.add_argument('--workers', default=8, type=int, help='Number of data loading workers.')\n",
    "parser.add_argument('--epochs', type=int, default=40, help='Total training epochs.')\n",
    "parser.add_argument('--num_head', type=int, default=4, help='Number of attention head.')\n",
    "parser.add_argument('--num_class', type=int, default=2, help='Number of class.')\n",
    "\n",
    "args = parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858ae0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): validation loss가 개선된 후 기다리는 기간\n",
    "                            Default: 7\n",
    "            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n",
    "                            Default: False\n",
    "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
    "                            Default: 0\n",
    "            path (str): checkpoint저장 경로\n",
    "                            Default: 'checkpoint.pt'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e3374c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(data.sampler.Sampler):\n",
    "    def __init__(self, dataset, indices: list = None, num_samples: int = None):\n",
    "        self.indices = list(range(len(dataset))) if indices is None else indices\n",
    "        self.num_samples = len(self.indices) if num_samples is None else num_samples\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df[\"label\"] = self._get_labels(dataset)\n",
    "        df.index = self.indices\n",
    "        df = df.sort_index()\n",
    "\n",
    "        label_to_count = df[\"label\"].value_counts()\n",
    "\n",
    "        weights = 1.0 / label_to_count[df[\"label\"]]\n",
    "\n",
    "        self.weights = torch.DoubleTensor(weights.to_list())\n",
    "\n",
    "        # self.weights = self.weights.clamp(min=1e-5)\n",
    "\n",
    "    def _get_labels(self, dataset):\n",
    "        if isinstance(dataset, datasets.ImageFolder):\n",
    "            return [x[1] for x in dataset.imgs]\n",
    "        elif isinstance(dataset, torch.utils.data.Subset):\n",
    "            return [dataset.dataset.imgs[i][1] for i in dataset.indices]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd9bce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole train set size: 60000\n"
     ]
    }
   ],
   "source": [
    "# define transformation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAffine(20, scale = (0.8, 1), translate = (0.2, 0.2)),], p = 0.7),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                        std = [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(),\n",
    "])\n",
    "\n",
    "# train_dataset\n",
    "train_dataset = datasets.ImageFolder('./datasets/AfectNet/train/pos', transform = data_transforms)\n",
    "if args.num_class == 3:\n",
    "    idx = [i for i in range(len(train_dataset)) if train_dataset.imgs[i][1] != 7]\n",
    "    train_dataset = data.Subset(train_dataset, idx)\n",
    "    \n",
    "print('Whole train set size:', train_dataset.__len__())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size = args.batch_size,\n",
    "                                          num_workers = args.workers,\n",
    "                                          sampler = ImbalancedDatasetSampler(train_dataset),\n",
    "                                          shuffle = False,\n",
    "                                          pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a85dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set size: 1250\n"
     ]
    }
   ],
   "source": [
    "data_transforms_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                        std = [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# val_dataset\n",
    "val_dataset = datasets.ImageFolder('./datasets/AfectNet/val/pos', transform = data_transforms_val)\n",
    "if args.num_class == 3:\n",
    "    idx = [i for i in range(len(val_dataset)) if val_dataset.imgs[i][1] != 7]\n",
    "    val_dataset = data.Subset(val_dataset, idx)\n",
    "    \n",
    "print('Validation set size:', val_dataset.__len__())\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = args.batch_size,\n",
    "    num_workers = args.workers,\n",
    "    shuffle = False,\n",
    "    pin_memory = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a13e6b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swish actiavtion function\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "    \n",
    "# # check\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.randn(3, 3, 224, 224)\n",
    "#     model = Swish()\n",
    "#     output = model(x)\n",
    "#     print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a25ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, r = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.squeeze = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // r),\n",
    "            Swish(),\n",
    "            nn.Linear(in_channels // r, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.excitation(x)\n",
    "        x = x.view(x.size(0), x.size(1), 1, 1)\n",
    "        return x\n",
    "    \n",
    "# # chech\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.randn(3, 56, 17, 17)\n",
    "#     model = SEBlock(x.size(1))\n",
    "#     output = model(x)\n",
    "#     print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73f17296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBConv(nn.Module):\n",
    "    expand = 6\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first MBConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * MBConv.expand, 1, stride=stride, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_channels * MBConv.expand, in_channels * MBConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*MBConv.expand),\n",
    "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * MBConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*MBConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# # check\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.randn(3, 16, 24, 24)\n",
    "#     model = MBConv(x.size(1), x.size(1), 3, stride=1, p=1)\n",
    "#     model.train()\n",
    "#     output = model(x)\n",
    "#     x = (output == x)\n",
    "#     print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "259616d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SepConv(nn.Module):\n",
    "    expand = 1\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first SepConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * SepConv.expand, in_channels * SepConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*SepConv.expand),\n",
    "            nn.BatchNorm2d(in_channels * SepConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * SepConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*SepConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# # check\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.randn(3, 16, 24, 24)\n",
    "#     model = SepConv(x.size(1), x.size(1), 3, stride=1, p=1)\n",
    "#     model.train()\n",
    "#     output = model(x)\n",
    "#     # stochastic depth check\n",
    "#     x = (output == x)\n",
    "#     print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "050ea5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, width_coef=1., depth_coef=1., scale=1., dropout=0.2, se_scale=4, stochastic_depth=False, p=0.5):\n",
    "        super().__init__()\n",
    "        channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
    "        repeats = [1, 2, 2, 3, 3, 4, 1]\n",
    "        strides = [1, 2, 2, 2, 1, 2, 1]\n",
    "        kernel_size = [3, 3, 5, 3, 5, 5, 3]\n",
    "        depth = depth_coef\n",
    "        width = width_coef\n",
    "\n",
    "        channels = [int(x*width) for x in channels]\n",
    "        repeats = [int(x*depth) for x in repeats]\n",
    "\n",
    "        # stochastic depth\n",
    "        if stochastic_depth:\n",
    "            self.p = p\n",
    "            self.step = (1 - 0.5) / (sum(repeats) - 1)\n",
    "        else:\n",
    "            self.p = 1\n",
    "            self.step = 0\n",
    "\n",
    "\n",
    "        # efficient net\n",
    "        self.upsample = nn.Upsample(scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(3, channels[0],3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(channels[0], momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.stage2 = self._make_Block(SepConv, repeats[0], channels[0], channels[1], kernel_size[0], strides[0], se_scale)\n",
    "\n",
    "        self.stage3 = self._make_Block(MBConv, repeats[1], channels[1], channels[2], kernel_size[1], strides[1], se_scale)\n",
    "\n",
    "        self.stage4 = self._make_Block(MBConv, repeats[2], channels[2], channels[3], kernel_size[2], strides[2], se_scale)\n",
    "\n",
    "        self.stage5 = self._make_Block(MBConv, repeats[3], channels[3], channels[4], kernel_size[3], strides[3], se_scale)\n",
    "\n",
    "        self.stage6 = self._make_Block(MBConv, repeats[4], channels[4], channels[5], kernel_size[4], strides[4], se_scale)\n",
    "\n",
    "        self.stage7 = self._make_Block(MBConv, repeats[5], channels[5], channels[6], kernel_size[5], strides[5], se_scale)\n",
    "\n",
    "        self.stage8 = self._make_Block(MBConv, repeats[6], channels[6], channels[7], kernel_size[6], strides[6], se_scale)\n",
    "\n",
    "        self.stage9 = nn.Sequential(\n",
    "            nn.Conv2d(channels[7], channels[8], 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(channels[8], momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        ) \n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear = nn.Linear(channels[8], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.type(torch.float16)\n",
    "        x = self.upsample(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.stage8(x)\n",
    "        x = self.stage9(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _make_Block(self, block, repeats, in_channels, out_channels, kernel_size, stride, se_scale):\n",
    "        strides = [stride] + [1] * (repeats - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(in_channels, out_channels, kernel_size, stride, se_scale, self.p))\n",
    "            in_channels = out_channels\n",
    "            self.p -= self.step\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def efficientnet_b0(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.0, scale=1.0,dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b1(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.1, scale=240/224, dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b2(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.1, depth_coef=1.2, scale=260/224., dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b3(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.2, depth_coef=1.4, scale=300/224, dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b4(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.4, depth_coef=1.8, scale=380/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b5(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.6, depth_coef=2.2, scale=456/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b6(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.8, depth_coef=2.6, scale=528/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "def efficientnet_b7(num_classes=2):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=2.0, depth_coef=3.1, scale=600/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "\n",
    "# # check\n",
    "# if __name__ == '__main__':\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     x = torch.randn(3, 3, 224, 224).to(device)\n",
    "#     model = efficientnet_b0().to(device)\n",
    "#     output = model(x)\n",
    "#     print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c634770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "          Upsample-1          [-1, 3, 260, 260]               0\n",
      "            Conv2d-2         [-1, 35, 130, 130]             945\n",
      "       BatchNorm2d-3         [-1, 35, 130, 130]              70\n",
      "            Conv2d-4         [-1, 35, 130, 130]             315\n",
      "       BatchNorm2d-5         [-1, 35, 130, 130]              70\n",
      "           Sigmoid-6         [-1, 35, 130, 130]               0\n",
      "             Swish-7         [-1, 35, 130, 130]               0\n",
      " AdaptiveAvgPool2d-8             [-1, 35, 1, 1]               0\n",
      "            Linear-9                    [-1, 8]             288\n",
      "          Sigmoid-10                    [-1, 8]               0\n",
      "            Swish-11                    [-1, 8]               0\n",
      "           Linear-12                   [-1, 35]             315\n",
      "          Sigmoid-13                   [-1, 35]               0\n",
      "          SEBlock-14             [-1, 35, 1, 1]               0\n",
      "           Conv2d-15         [-1, 17, 130, 130]             595\n",
      "      BatchNorm2d-16         [-1, 17, 130, 130]              34\n",
      "          SepConv-17         [-1, 17, 130, 130]               0\n",
      "           Conv2d-18          [-1, 102, 65, 65]           1,734\n",
      "      BatchNorm2d-19          [-1, 102, 65, 65]             204\n",
      "          Sigmoid-20          [-1, 102, 65, 65]               0\n",
      "            Swish-21          [-1, 102, 65, 65]               0\n",
      "           Conv2d-22          [-1, 102, 65, 65]             918\n",
      "      BatchNorm2d-23          [-1, 102, 65, 65]             204\n",
      "          Sigmoid-24          [-1, 102, 65, 65]               0\n",
      "            Swish-25          [-1, 102, 65, 65]               0\n",
      "AdaptiveAvgPool2d-26            [-1, 102, 1, 1]               0\n",
      "           Linear-27                   [-1, 25]           2,575\n",
      "          Sigmoid-28                   [-1, 25]               0\n",
      "            Swish-29                   [-1, 25]               0\n",
      "           Linear-30                  [-1, 102]           2,652\n",
      "          Sigmoid-31                  [-1, 102]               0\n",
      "          SEBlock-32            [-1, 102, 1, 1]               0\n",
      "           Conv2d-33           [-1, 26, 65, 65]           2,652\n",
      "      BatchNorm2d-34           [-1, 26, 65, 65]              52\n",
      "           MBConv-35           [-1, 26, 65, 65]               0\n",
      "           Conv2d-36          [-1, 156, 65, 65]           4,056\n",
      "      BatchNorm2d-37          [-1, 156, 65, 65]             312\n",
      "          Sigmoid-38          [-1, 156, 65, 65]               0\n",
      "            Swish-39          [-1, 156, 65, 65]               0\n",
      "           Conv2d-40          [-1, 156, 65, 65]           1,404\n",
      "      BatchNorm2d-41          [-1, 156, 65, 65]             312\n",
      "          Sigmoid-42          [-1, 156, 65, 65]               0\n",
      "            Swish-43          [-1, 156, 65, 65]               0\n",
      "AdaptiveAvgPool2d-44            [-1, 156, 1, 1]               0\n",
      "           Linear-45                   [-1, 39]           6,123\n",
      "          Sigmoid-46                   [-1, 39]               0\n",
      "            Swish-47                   [-1, 39]               0\n",
      "           Linear-48                  [-1, 156]           6,240\n",
      "          Sigmoid-49                  [-1, 156]               0\n",
      "          SEBlock-50            [-1, 156, 1, 1]               0\n",
      "           Conv2d-51           [-1, 26, 65, 65]           4,056\n",
      "      BatchNorm2d-52           [-1, 26, 65, 65]              52\n",
      "           MBConv-53           [-1, 26, 65, 65]               0\n",
      "           Conv2d-54          [-1, 156, 33, 33]           4,056\n",
      "      BatchNorm2d-55          [-1, 156, 33, 33]             312\n",
      "          Sigmoid-56          [-1, 156, 33, 33]               0\n",
      "            Swish-57          [-1, 156, 33, 33]               0\n",
      "           Conv2d-58          [-1, 156, 33, 33]           3,900\n",
      "      BatchNorm2d-59          [-1, 156, 33, 33]             312\n",
      "          Sigmoid-60          [-1, 156, 33, 33]               0\n",
      "            Swish-61          [-1, 156, 33, 33]               0\n",
      "AdaptiveAvgPool2d-62            [-1, 156, 1, 1]               0\n",
      "           Linear-63                   [-1, 39]           6,123\n",
      "          Sigmoid-64                   [-1, 39]               0\n",
      "            Swish-65                   [-1, 39]               0\n",
      "           Linear-66                  [-1, 156]           6,240\n",
      "          Sigmoid-67                  [-1, 156]               0\n",
      "          SEBlock-68            [-1, 156, 1, 1]               0\n",
      "           Conv2d-69           [-1, 44, 33, 33]           6,864\n",
      "      BatchNorm2d-70           [-1, 44, 33, 33]              88\n",
      "           MBConv-71           [-1, 44, 33, 33]               0\n",
      "           Conv2d-72          [-1, 264, 33, 33]          11,616\n",
      "      BatchNorm2d-73          [-1, 264, 33, 33]             528\n",
      "          Sigmoid-74          [-1, 264, 33, 33]               0\n",
      "            Swish-75          [-1, 264, 33, 33]               0\n",
      "           Conv2d-76          [-1, 264, 33, 33]           6,600\n",
      "      BatchNorm2d-77          [-1, 264, 33, 33]             528\n",
      "          Sigmoid-78          [-1, 264, 33, 33]               0\n",
      "            Swish-79          [-1, 264, 33, 33]               0\n",
      "AdaptiveAvgPool2d-80            [-1, 264, 1, 1]               0\n",
      "           Linear-81                   [-1, 66]          17,490\n",
      "          Sigmoid-82                   [-1, 66]               0\n",
      "            Swish-83                   [-1, 66]               0\n",
      "           Linear-84                  [-1, 264]          17,688\n",
      "          Sigmoid-85                  [-1, 264]               0\n",
      "          SEBlock-86            [-1, 264, 1, 1]               0\n",
      "           Conv2d-87           [-1, 44, 33, 33]          11,616\n",
      "      BatchNorm2d-88           [-1, 44, 33, 33]              88\n",
      "           MBConv-89           [-1, 44, 33, 33]               0\n",
      "           Conv2d-90          [-1, 264, 17, 17]          11,616\n",
      "      BatchNorm2d-91          [-1, 264, 17, 17]             528\n",
      "          Sigmoid-92          [-1, 264, 17, 17]               0\n",
      "            Swish-93          [-1, 264, 17, 17]               0\n",
      "           Conv2d-94          [-1, 264, 17, 17]           2,376\n",
      "      BatchNorm2d-95          [-1, 264, 17, 17]             528\n",
      "          Sigmoid-96          [-1, 264, 17, 17]               0\n",
      "            Swish-97          [-1, 264, 17, 17]               0\n",
      "AdaptiveAvgPool2d-98            [-1, 264, 1, 1]               0\n",
      "           Linear-99                   [-1, 66]          17,490\n",
      "         Sigmoid-100                   [-1, 66]               0\n",
      "           Swish-101                   [-1, 66]               0\n",
      "          Linear-102                  [-1, 264]          17,688\n",
      "         Sigmoid-103                  [-1, 264]               0\n",
      "         SEBlock-104            [-1, 264, 1, 1]               0\n",
      "          Conv2d-105           [-1, 88, 17, 17]          23,232\n",
      "     BatchNorm2d-106           [-1, 88, 17, 17]             176\n",
      "          MBConv-107           [-1, 88, 17, 17]               0\n",
      "          Conv2d-108          [-1, 528, 17, 17]          46,464\n",
      "     BatchNorm2d-109          [-1, 528, 17, 17]           1,056\n",
      "         Sigmoid-110          [-1, 528, 17, 17]               0\n",
      "           Swish-111          [-1, 528, 17, 17]               0\n",
      "          Conv2d-112          [-1, 528, 17, 17]           4,752\n",
      "     BatchNorm2d-113          [-1, 528, 17, 17]           1,056\n",
      "         Sigmoid-114          [-1, 528, 17, 17]               0\n",
      "           Swish-115          [-1, 528, 17, 17]               0\n",
      "AdaptiveAvgPool2d-116            [-1, 528, 1, 1]               0\n",
      "          Linear-117                  [-1, 132]          69,828\n",
      "         Sigmoid-118                  [-1, 132]               0\n",
      "           Swish-119                  [-1, 132]               0\n",
      "          Linear-120                  [-1, 528]          70,224\n",
      "         Sigmoid-121                  [-1, 528]               0\n",
      "         SEBlock-122            [-1, 528, 1, 1]               0\n",
      "          Conv2d-123           [-1, 88, 17, 17]          46,464\n",
      "     BatchNorm2d-124           [-1, 88, 17, 17]             176\n",
      "          MBConv-125           [-1, 88, 17, 17]               0\n",
      "          Conv2d-126          [-1, 528, 17, 17]          46,464\n",
      "     BatchNorm2d-127          [-1, 528, 17, 17]           1,056\n",
      "         Sigmoid-128          [-1, 528, 17, 17]               0\n",
      "           Swish-129          [-1, 528, 17, 17]               0\n",
      "          Conv2d-130          [-1, 528, 17, 17]           4,752\n",
      "     BatchNorm2d-131          [-1, 528, 17, 17]           1,056\n",
      "         Sigmoid-132          [-1, 528, 17, 17]               0\n",
      "           Swish-133          [-1, 528, 17, 17]               0\n",
      "AdaptiveAvgPool2d-134            [-1, 528, 1, 1]               0\n",
      "          Linear-135                  [-1, 132]          69,828\n",
      "         Sigmoid-136                  [-1, 132]               0\n",
      "           Swish-137                  [-1, 132]               0\n",
      "          Linear-138                  [-1, 528]          70,224\n",
      "         Sigmoid-139                  [-1, 528]               0\n",
      "         SEBlock-140            [-1, 528, 1, 1]               0\n",
      "          Conv2d-141           [-1, 88, 17, 17]          46,464\n",
      "     BatchNorm2d-142           [-1, 88, 17, 17]             176\n",
      "          MBConv-143           [-1, 88, 17, 17]               0\n",
      "          Conv2d-144          [-1, 528, 17, 17]          46,464\n",
      "     BatchNorm2d-145          [-1, 528, 17, 17]           1,056\n",
      "         Sigmoid-146          [-1, 528, 17, 17]               0\n",
      "           Swish-147          [-1, 528, 17, 17]               0\n",
      "          Conv2d-148          [-1, 528, 17, 17]          13,200\n",
      "     BatchNorm2d-149          [-1, 528, 17, 17]           1,056\n",
      "         Sigmoid-150          [-1, 528, 17, 17]               0\n",
      "           Swish-151          [-1, 528, 17, 17]               0\n",
      "AdaptiveAvgPool2d-152            [-1, 528, 1, 1]               0\n",
      "          Linear-153                  [-1, 132]          69,828\n",
      "         Sigmoid-154                  [-1, 132]               0\n",
      "           Swish-155                  [-1, 132]               0\n",
      "          Linear-156                  [-1, 528]          70,224\n",
      "         Sigmoid-157                  [-1, 528]               0\n",
      "         SEBlock-158            [-1, 528, 1, 1]               0\n",
      "          Conv2d-159          [-1, 123, 17, 17]          64,944\n",
      "     BatchNorm2d-160          [-1, 123, 17, 17]             246\n",
      "          MBConv-161          [-1, 123, 17, 17]               0\n",
      "          Conv2d-162          [-1, 738, 17, 17]          90,774\n",
      "     BatchNorm2d-163          [-1, 738, 17, 17]           1,476\n",
      "         Sigmoid-164          [-1, 738, 17, 17]               0\n",
      "           Swish-165          [-1, 738, 17, 17]               0\n",
      "          Conv2d-166          [-1, 738, 17, 17]          18,450\n",
      "     BatchNorm2d-167          [-1, 738, 17, 17]           1,476\n",
      "         Sigmoid-168          [-1, 738, 17, 17]               0\n",
      "           Swish-169          [-1, 738, 17, 17]               0\n",
      "AdaptiveAvgPool2d-170            [-1, 738, 1, 1]               0\n",
      "          Linear-171                  [-1, 184]         135,976\n",
      "         Sigmoid-172                  [-1, 184]               0\n",
      "           Swish-173                  [-1, 184]               0\n",
      "          Linear-174                  [-1, 738]         136,530\n",
      "         Sigmoid-175                  [-1, 738]               0\n",
      "         SEBlock-176            [-1, 738, 1, 1]               0\n",
      "          Conv2d-177          [-1, 123, 17, 17]          90,774\n",
      "     BatchNorm2d-178          [-1, 123, 17, 17]             246\n",
      "          MBConv-179          [-1, 123, 17, 17]               0\n",
      "          Conv2d-180          [-1, 738, 17, 17]          90,774\n",
      "     BatchNorm2d-181          [-1, 738, 17, 17]           1,476\n",
      "         Sigmoid-182          [-1, 738, 17, 17]               0\n",
      "           Swish-183          [-1, 738, 17, 17]               0\n",
      "          Conv2d-184          [-1, 738, 17, 17]          18,450\n",
      "     BatchNorm2d-185          [-1, 738, 17, 17]           1,476\n",
      "         Sigmoid-186          [-1, 738, 17, 17]               0\n",
      "           Swish-187          [-1, 738, 17, 17]               0\n",
      "AdaptiveAvgPool2d-188            [-1, 738, 1, 1]               0\n",
      "          Linear-189                  [-1, 184]         135,976\n",
      "         Sigmoid-190                  [-1, 184]               0\n",
      "           Swish-191                  [-1, 184]               0\n",
      "          Linear-192                  [-1, 738]         136,530\n",
      "         Sigmoid-193                  [-1, 738]               0\n",
      "         SEBlock-194            [-1, 738, 1, 1]               0\n",
      "          Conv2d-195          [-1, 123, 17, 17]          90,774\n",
      "     BatchNorm2d-196          [-1, 123, 17, 17]             246\n",
      "          MBConv-197          [-1, 123, 17, 17]               0\n",
      "          Conv2d-198            [-1, 738, 9, 9]          90,774\n",
      "     BatchNorm2d-199            [-1, 738, 9, 9]           1,476\n",
      "         Sigmoid-200            [-1, 738, 9, 9]               0\n",
      "           Swish-201            [-1, 738, 9, 9]               0\n",
      "          Conv2d-202            [-1, 738, 9, 9]          18,450\n",
      "     BatchNorm2d-203            [-1, 738, 9, 9]           1,476\n",
      "         Sigmoid-204            [-1, 738, 9, 9]               0\n",
      "           Swish-205            [-1, 738, 9, 9]               0\n",
      "AdaptiveAvgPool2d-206            [-1, 738, 1, 1]               0\n",
      "          Linear-207                  [-1, 184]         135,976\n",
      "         Sigmoid-208                  [-1, 184]               0\n",
      "           Swish-209                  [-1, 184]               0\n",
      "          Linear-210                  [-1, 738]         136,530\n",
      "         Sigmoid-211                  [-1, 738]               0\n",
      "         SEBlock-212            [-1, 738, 1, 1]               0\n",
      "          Conv2d-213            [-1, 211, 9, 9]         155,718\n",
      "     BatchNorm2d-214            [-1, 211, 9, 9]             422\n",
      "          MBConv-215            [-1, 211, 9, 9]               0\n",
      "          Conv2d-216           [-1, 1266, 9, 9]         267,126\n",
      "     BatchNorm2d-217           [-1, 1266, 9, 9]           2,532\n",
      "         Sigmoid-218           [-1, 1266, 9, 9]               0\n",
      "           Swish-219           [-1, 1266, 9, 9]               0\n",
      "          Conv2d-220           [-1, 1266, 9, 9]          31,650\n",
      "     BatchNorm2d-221           [-1, 1266, 9, 9]           2,532\n",
      "         Sigmoid-222           [-1, 1266, 9, 9]               0\n",
      "           Swish-223           [-1, 1266, 9, 9]               0\n",
      "AdaptiveAvgPool2d-224           [-1, 1266, 1, 1]               0\n",
      "          Linear-225                  [-1, 316]         400,372\n",
      "         Sigmoid-226                  [-1, 316]               0\n",
      "           Swish-227                  [-1, 316]               0\n",
      "          Linear-228                 [-1, 1266]         401,322\n",
      "         Sigmoid-229                 [-1, 1266]               0\n",
      "         SEBlock-230           [-1, 1266, 1, 1]               0\n",
      "          Conv2d-231            [-1, 211, 9, 9]         267,126\n",
      "     BatchNorm2d-232            [-1, 211, 9, 9]             422\n",
      "          MBConv-233            [-1, 211, 9, 9]               0\n",
      "          Conv2d-234           [-1, 1266, 9, 9]         267,126\n",
      "     BatchNorm2d-235           [-1, 1266, 9, 9]           2,532\n",
      "         Sigmoid-236           [-1, 1266, 9, 9]               0\n",
      "           Swish-237           [-1, 1266, 9, 9]               0\n",
      "          Conv2d-238           [-1, 1266, 9, 9]          31,650\n",
      "     BatchNorm2d-239           [-1, 1266, 9, 9]           2,532\n",
      "         Sigmoid-240           [-1, 1266, 9, 9]               0\n",
      "           Swish-241           [-1, 1266, 9, 9]               0\n",
      "AdaptiveAvgPool2d-242           [-1, 1266, 1, 1]               0\n",
      "          Linear-243                  [-1, 316]         400,372\n",
      "         Sigmoid-244                  [-1, 316]               0\n",
      "           Swish-245                  [-1, 316]               0\n",
      "          Linear-246                 [-1, 1266]         401,322\n",
      "         Sigmoid-247                 [-1, 1266]               0\n",
      "         SEBlock-248           [-1, 1266, 1, 1]               0\n",
      "          Conv2d-249            [-1, 211, 9, 9]         267,126\n",
      "     BatchNorm2d-250            [-1, 211, 9, 9]             422\n",
      "          MBConv-251            [-1, 211, 9, 9]               0\n",
      "          Conv2d-252           [-1, 1266, 9, 9]         267,126\n",
      "     BatchNorm2d-253           [-1, 1266, 9, 9]           2,532\n",
      "         Sigmoid-254           [-1, 1266, 9, 9]               0\n",
      "           Swish-255           [-1, 1266, 9, 9]               0\n",
      "          Conv2d-256           [-1, 1266, 9, 9]          31,650\n",
      "     BatchNorm2d-257           [-1, 1266, 9, 9]           2,532\n",
      "         Sigmoid-258           [-1, 1266, 9, 9]               0\n",
      "           Swish-259           [-1, 1266, 9, 9]               0\n",
      "AdaptiveAvgPool2d-260           [-1, 1266, 1, 1]               0\n",
      "          Linear-261                  [-1, 316]         400,372\n",
      "         Sigmoid-262                  [-1, 316]               0\n",
      "           Swish-263                  [-1, 316]               0\n",
      "          Linear-264                 [-1, 1266]         401,322\n",
      "         Sigmoid-265                 [-1, 1266]               0\n",
      "         SEBlock-266           [-1, 1266, 1, 1]               0\n",
      "          Conv2d-267            [-1, 211, 9, 9]         267,126\n",
      "     BatchNorm2d-268            [-1, 211, 9, 9]             422\n",
      "          MBConv-269            [-1, 211, 9, 9]               0\n",
      "          Conv2d-270           [-1, 1266, 9, 9]         267,126\n",
      "     BatchNorm2d-271           [-1, 1266, 9, 9]           2,532\n",
      "         Sigmoid-272           [-1, 1266, 9, 9]               0\n",
      "           Swish-273           [-1, 1266, 9, 9]               0\n",
      "          Conv2d-274           [-1, 1266, 9, 9]          11,394\n",
      "     BatchNorm2d-275           [-1, 1266, 9, 9]           2,532\n",
      "         Sigmoid-276           [-1, 1266, 9, 9]               0\n",
      "           Swish-277           [-1, 1266, 9, 9]               0\n",
      "AdaptiveAvgPool2d-278           [-1, 1266, 1, 1]               0\n",
      "          Linear-279                  [-1, 316]         400,372\n",
      "         Sigmoid-280                  [-1, 316]               0\n",
      "           Swish-281                  [-1, 316]               0\n",
      "          Linear-282                 [-1, 1266]         401,322\n",
      "         Sigmoid-283                 [-1, 1266]               0\n",
      "         SEBlock-284           [-1, 1266, 1, 1]               0\n",
      "          Conv2d-285            [-1, 352, 9, 9]         445,632\n",
      "     BatchNorm2d-286            [-1, 352, 9, 9]             704\n",
      "          MBConv-287            [-1, 352, 9, 9]               0\n",
      "          Conv2d-288           [-1, 1408, 9, 9]         495,616\n",
      "     BatchNorm2d-289           [-1, 1408, 9, 9]           2,816\n",
      "         Sigmoid-290           [-1, 1408, 9, 9]               0\n",
      "           Swish-291           [-1, 1408, 9, 9]               0\n",
      "AdaptiveAvgPool2d-292           [-1, 1408, 1, 1]               0\n",
      "         Dropout-293                 [-1, 1408]               0\n",
      "          Linear-294                    [-1, 2]           2,818\n",
      "================================================================\n",
      "Total params: 8,595,255\n",
      "Trainable params: 8,595,255\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 234.45\n",
      "Params size (MB): 32.79\n",
      "Estimated Total Size (MB): 267.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print model summary\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = efficientnet_b2().to(device)\n",
    "summary(model, (3,224,224), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "988dec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function, optimizer, lr_scheduler\n",
    "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "#############################\n",
    "f1 = F1Score(num_classes = 2)\n",
    "#############################\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "\n",
    "# get current lr\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "# calculate the metric per mini-batch\n",
    "def metric_batch(output, target):\n",
    "    pred = output.argmax(1, keepdim=True)\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "# calculate the loss per mini-batch\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss_b = loss_func(output, target)\n",
    "    metric_b = metric_batch(output, target)\n",
    "\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss_b.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    return loss_b.item(), metric_b\n",
    "\n",
    "\n",
    "# calculate the loss per epochs\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "\n",
    "    for xb, yb in dataset_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        output = model(xb)\n",
    "\n",
    "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
    "\n",
    "        running_loss += loss_b\n",
    "        \n",
    "        if metric_b is not None:\n",
    "            running_metric += metric_b\n",
    "\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "    loss = running_loss / len_data\n",
    "    metric = running_metric / len_data\n",
    "    return loss, metric\n",
    "\n",
    "\n",
    "# function to start training\n",
    "def train_val(model, params):\n",
    "    num_epochs=params['num_epochs']\n",
    "    loss_func=params['loss_func']\n",
    "    opt=params['optimizer']\n",
    "    train_dl=params['train_dl']\n",
    "    val_dl=params['val_dl']\n",
    "    sanity_check=params['sanity_check']\n",
    "    lr_scheduler=params['lr_scheduler']\n",
    "    path2weights1=params['path2weights1']\n",
    "    path2weights2=params['path2weights2']\n",
    "\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    metric_history = {'train': [], 'val': []}\n",
    "    ####################################################################\n",
    "    early_stopping = EarlyStopping(patience = 10, verbose = True)\n",
    "    ####################################################################\n",
    "    best_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        metric_history['train'].append(train_metric)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "            #######################################################\n",
    "#             f1_score = 0\n",
    "#             for k, (img, target) in enumerate(val_dl):\n",
    "#                 img, target = img.to(device), target.to(device)\n",
    "                \n",
    "#                 output = model(img).to(device)\n",
    "#                 pred = output.argmax(dim = 1).to(device)\n",
    "#                 target = target.view_as(pred).to(device)\n",
    "                \n",
    "#                 f1_score += f1(pred, target).to(device)\n",
    "            \n",
    "#             print(\"Validation F1 Score: \", f1_score)\n",
    "            ########################################################\n",
    "        loss_history['val'].append(val_loss)\n",
    "        metric_history['val'].append(val_metric)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model, path2weights1)\n",
    "            torch.save(model.state_dict(), path2weights2)\n",
    "            print('Copied best model weights!')\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print('Loading best model weights!')\n",
    "            model.load_state_dict(best_model_wts)\n",
    "\n",
    "        #############\n",
    "        early_stopping(val_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        #############\n",
    "        \n",
    "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
    "        print('-'*10)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69e6c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training parameters\n",
    "params_train = {\n",
    "    'num_epochs':100,\n",
    "    'optimizer':opt,\n",
    "    'loss_func':loss_func,\n",
    "    'train_dl':train_loader,\n",
    "    'val_dl':val_loader,\n",
    "    'sanity_check':False,\n",
    "    'lr_scheduler':lr_scheduler,\n",
    "    'path2weights1':'./models/pos_weights_all.pt',\n",
    "    'path2weights2':'./models/pos_weights_only_parameters.pt'\n",
    "}\n",
    "\n",
    "# check the directory to save weights.pt\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSerror:\n",
    "        print('Error')\n",
    "createFolder('./models_b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "427a4eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99, current lr= 0.01\n",
      "Copied best model weights!\n",
      "Validation loss decreased (inf --> 0.718569).  Saving model ...\n",
      "train loss: 0.701130, val loss: 0.718569, accuracy: 62.16, time: 7.0537 min\n",
      "----------\n",
      "Epoch 1/99, current lr= 0.01\n",
      "EarlyStopping counter: 1 out of 10\n",
      "train loss: 0.575130, val loss: 0.730148, accuracy: 63.92, time: 14.1511 min\n",
      "----------\n",
      "Epoch 2/99, current lr= 0.01\n",
      "Copied best model weights!\n",
      "Validation loss decreased (0.718569 --> 0.608119).  Saving model ...\n",
      "train loss: 0.444923, val loss: 0.608119, accuracy: 65.76, time: 21.3045 min\n",
      "----------\n",
      "Epoch 3/99, current lr= 0.01\n",
      "EarlyStopping counter: 1 out of 10\n",
      "train loss: 0.412562, val loss: 0.676350, accuracy: 66.32, time: 28.5261 min\n",
      "----------\n",
      "Epoch 4/99, current lr= 0.01\n",
      "EarlyStopping counter: 2 out of 10\n",
      "train loss: 0.388044, val loss: 0.926399, accuracy: 59.92, time: 36.1393 min\n",
      "----------\n",
      "Epoch 5/99, current lr= 0.01\n",
      "EarlyStopping counter: 3 out of 10\n",
      "train loss: 0.382315, val loss: 0.639958, accuracy: 68.24, time: 43.7762 min\n",
      "----------\n",
      "Epoch 6/99, current lr= 0.01\n",
      "EarlyStopping counter: 4 out of 10\n",
      "train loss: 0.370933, val loss: 0.690300, accuracy: 69.36, time: 51.1730 min\n",
      "----------\n",
      "Epoch 7/99, current lr= 0.01\n",
      "EarlyStopping counter: 5 out of 10\n",
      "train loss: 0.363846, val loss: 0.913793, accuracy: 64.48, time: 58.6499 min\n",
      "----------\n",
      "Epoch 8/99, current lr= 0.01\n",
      "EarlyStopping counter: 6 out of 10\n",
      "train loss: 0.358373, val loss: 0.641704, accuracy: 69.84, time: 66.3008 min\n",
      "----------\n",
      "Epoch 9/99, current lr= 0.01\n",
      "EarlyStopping counter: 7 out of 10\n",
      "train loss: 0.351331, val loss: 0.627440, accuracy: 68.80, time: 73.9085 min\n",
      "----------\n",
      "Epoch 10/99, current lr= 0.01\n",
      "Copied best model weights!\n",
      "Validation loss decreased (0.608119 --> 0.556016).  Saving model ...\n",
      "train loss: 0.346820, val loss: 0.556016, accuracy: 72.88, time: 81.5189 min\n",
      "----------\n",
      "Epoch 11/99, current lr= 0.01\n",
      "EarlyStopping counter: 1 out of 10\n",
      "train loss: 0.343569, val loss: 0.703722, accuracy: 70.64, time: 89.0703 min\n",
      "----------\n",
      "Epoch 12/99, current lr= 0.01\n",
      "Copied best model weights!\n",
      "Validation loss decreased (0.556016 --> 0.409075).  Saving model ...\n",
      "train loss: 0.340139, val loss: 0.409075, accuracy: 79.76, time: 96.6636 min\n",
      "----------\n",
      "Epoch 13/99, current lr= 0.01\n",
      "EarlyStopping counter: 1 out of 10\n",
      "train loss: 0.332863, val loss: 0.466723, accuracy: 74.40, time: 104.2392 min\n",
      "----------\n",
      "Epoch 14/99, current lr= 0.01\n",
      "EarlyStopping counter: 2 out of 10\n",
      "train loss: 0.331420, val loss: 0.443233, accuracy: 77.84, time: 111.8446 min\n",
      "----------\n",
      "Epoch 15/99, current lr= 0.01\n",
      "Copied best model weights!\n",
      "Validation loss decreased (0.409075 --> 0.408433).  Saving model ...\n",
      "train loss: 0.336892, val loss: 0.408433, accuracy: 78.56, time: 119.3579 min\n",
      "----------\n",
      "Epoch 16/99, current lr= 0.01\n",
      "EarlyStopping counter: 1 out of 10\n",
      "train loss: 0.325686, val loss: 0.504793, accuracy: 73.28, time: 126.9251 min\n",
      "----------\n",
      "Epoch 17/99, current lr= 0.01\n",
      "EarlyStopping counter: 2 out of 10\n",
      "train loss: 0.323279, val loss: 0.677375, accuracy: 73.76, time: 134.8461 min\n",
      "----------\n",
      "Epoch 18/99, current lr= 0.01\n",
      "EarlyStopping counter: 3 out of 10\n",
      "train loss: 0.321286, val loss: 0.504308, accuracy: 74.32, time: 143.1588 min\n",
      "----------\n",
      "Epoch 19/99, current lr= 0.01\n",
      "EarlyStopping counter: 4 out of 10\n",
      "train loss: 0.323762, val loss: 0.441553, accuracy: 78.80, time: 151.1115 min\n",
      "----------\n",
      "Epoch 20/99, current lr= 0.01\n",
      "EarlyStopping counter: 5 out of 10\n",
      "train loss: 0.315827, val loss: 0.442114, accuracy: 77.92, time: 159.0672 min\n",
      "----------\n",
      "Epoch 21/99, current lr= 0.01\n",
      "EarlyStopping counter: 6 out of 10\n",
      "train loss: 0.323052, val loss: 0.516291, accuracy: 73.52, time: 167.0048 min\n",
      "----------\n",
      "Epoch 22/99, current lr= 0.01\n",
      "Copied best model weights!\n",
      "Validation loss decreased (0.408433 --> 0.407965).  Saving model ...\n",
      "train loss: 0.311474, val loss: 0.407965, accuracy: 79.52, time: 175.1280 min\n",
      "----------\n",
      "Epoch 23/99, current lr= 0.01\n",
      "EarlyStopping counter: 1 out of 10\n",
      "train loss: 0.315372, val loss: 0.609533, accuracy: 75.36, time: 183.1326 min\n",
      "----------\n",
      "Epoch 24/99, current lr= 0.01\n",
      "EarlyStopping counter: 2 out of 10\n",
      "train loss: 0.314268, val loss: 0.569138, accuracy: 74.08, time: 191.1207 min\n",
      "----------\n",
      "Epoch 25/99, current lr= 0.01\n",
      "EarlyStopping counter: 3 out of 10\n",
      "train loss: 0.314816, val loss: 0.580986, accuracy: 75.04, time: 198.9125 min\n",
      "----------\n",
      "Epoch 26/99, current lr= 0.01\n",
      "EarlyStopping counter: 4 out of 10\n",
      "train loss: 0.306865, val loss: 0.459324, accuracy: 77.68, time: 206.8770 min\n",
      "----------\n",
      "Epoch 27/99, current lr= 0.01\n",
      "EarlyStopping counter: 5 out of 10\n",
      "train loss: 0.307694, val loss: 0.739445, accuracy: 72.48, time: 214.6995 min\n",
      "----------\n",
      "Epoch 28/99, current lr= 0.01\n",
      "EarlyStopping counter: 6 out of 10\n",
      "train loss: 0.308917, val loss: 0.530936, accuracy: 76.40, time: 222.5262 min\n",
      "----------\n",
      "Epoch 29/99, current lr= 0.01\n",
      "Copied best model weights!\n",
      "Validation loss decreased (0.407965 --> 0.380591).  Saving model ...\n",
      "train loss: 0.307059, val loss: 0.380591, accuracy: 81.52, time: 230.4872 min\n",
      "----------\n",
      "Epoch 30/99, current lr= 0.01\n",
      "EarlyStopping counter: 1 out of 10\n",
      "train loss: 0.308804, val loss: 0.556470, accuracy: 76.40, time: 238.3438 min\n",
      "----------\n",
      "Epoch 31/99, current lr= 0.01\n",
      "EarlyStopping counter: 2 out of 10\n",
      "train loss: 0.298876, val loss: 0.703657, accuracy: 72.88, time: 246.1084 min\n",
      "----------\n",
      "Epoch 32/99, current lr= 0.01\n",
      "EarlyStopping counter: 3 out of 10\n",
      "train loss: 0.299296, val loss: 0.403253, accuracy: 81.68, time: 253.8758 min\n",
      "----------\n",
      "Epoch 33/99, current lr= 0.01\n",
      "EarlyStopping counter: 4 out of 10\n",
      "train loss: 0.298999, val loss: 0.440350, accuracy: 79.12, time: 261.7955 min\n",
      "----------\n",
      "Epoch 34/99, current lr= 0.01\n",
      "EarlyStopping counter: 5 out of 10\n",
      "train loss: 0.300393, val loss: 0.520636, accuracy: 76.48, time: 269.6372 min\n",
      "----------\n",
      "Epoch 35/99, current lr= 0.01\n",
      "EarlyStopping counter: 6 out of 10\n",
      "train loss: 0.307256, val loss: 0.491733, accuracy: 78.16, time: 277.6535 min\n",
      "----------\n",
      "Epoch 36/99, current lr= 0.01\n",
      "EarlyStopping counter: 7 out of 10\n",
      "train loss: 0.297711, val loss: 0.436450, accuracy: 80.16, time: 285.6552 min\n",
      "----------\n",
      "Epoch 37/99, current lr= 0.01\n",
      "EarlyStopping counter: 8 out of 10\n",
      "train loss: 0.297525, val loss: 0.475692, accuracy: 79.76, time: 293.5067 min\n",
      "----------\n",
      "Epoch 38/99, current lr= 0.01\n",
      "EarlyStopping counter: 9 out of 10\n",
      "train loss: 0.300528, val loss: 0.420086, accuracy: 80.00, time: 301.0387 min\n",
      "----------\n",
      "Epoch 39/99, current lr= 0.01\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "model, loss_hist, metric_hist = train_val(model, params_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
